{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying YSOs with a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PyMultiNest not imported.  MultiNest fits will not work.\n"
     ]
    }
   ],
   "source": [
    "#ML imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#System/general imports\n",
    "import math\n",
    "import imf\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')\n",
    "import time\n",
    "\n",
    "#Data imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# import tables\n",
    "\n",
    "#Astro imports\n",
    "from astropy.io import fits\n",
    "from isochrones import parsec\n",
    "from isochrones.mist import MIST_Isochrone\n",
    "from isochrones.parsec import Parsec_Isochrone\n",
    "mist = Parsec_Isochrone()\n",
    "\n",
    "def getmagerror(flux,eflux):\n",
    "    return (-2.5*np.log10(flux-eflux)+2.5*np.log10(flux+eflux))/2\n",
    "\n",
    "\n",
    "# device = torch.device('cuda:0')\n",
    "\n",
    "# import pymultinest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88089, 6])\n",
      "torch.Size([87564, 6])\n",
      "torch.Size([88112, 6])\n"
     ]
    }
   ],
   "source": [
    "clusterx ,clustery =pickle.load(open('ysotrainf1ms.pickle', 'rb'))\n",
    "clusterx1,clustery1=pickle.load(open('ysotrainf2ms.pickle', 'rb'))\n",
    "clusterx2,clustery2=pickle.load(open('ysotrainf3ms.pickle', 'rb'))\n",
    "\n",
    "print(clustery.size())\n",
    "print(clustery1.size())\n",
    "print(clustery2.size())\n",
    "\n",
    "# print(-2.5*np.log10(10**(-mist.mag['G'](mass, 8, feh, distance, AV)/2.5)))\n",
    "# print(-2.5*np.log10(10**(-mist.mag['G'](mass, 8.3, feh, distance, AV)/2.5)))\n",
    "# print(-2.5*np.log10(10**(-mist.mag['G'](mass, 7.5, feh, distance, AV)/2.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to generate the clusters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "size = 300000\n",
    "mass = np.random.uniform(low=.08,high=3,size=size)\n",
    "age = np.random.uniform(low=6, high=10, size = size)\n",
    "feh = np.random.uniform(low=-2.5, high = .5, size = size)\n",
    "distance = np.random.uniform(low = 50, high = 1000, size = size)\n",
    "AV = np.random.uniform(low=0, high = 20, size = size)\n",
    "\n",
    "clx = np.recarray(size, \n",
    "                 dtype= [('g', float), ('bp', float), ('rp', float), ('j', float),\n",
    "                         ('h', float), ('k', float), ('w1', float), ('w2', float),\n",
    "                         ('w3', float), ('parallax', float),('mass', float),\n",
    "                         ('massratio',float), ('age',float), ('feh', float),\n",
    "                         ('av',float),('teff', float), ('logg', float),\n",
    "                         ('distance', float), ('logl', float), ('radius', float)])\n",
    "clx.mass = mass #Intrinsic properterties - random\n",
    "clx.age = age\n",
    "clx.feh = feh\n",
    "clx.av = AV \n",
    "clx.distance=distance\n",
    "\n",
    "\n",
    "clx.g=-2.5*np.log10(10**(-mist.mag['G'](mass, age, feh, distance, AV)/2.5))\n",
    "#test=-2.5*np.log10(10**(-mist.mag['G'](mass, 8, feh, distance, AV)/2.5))\n",
    "clx.bp=-2.5*np.log10(10**(-mist.mag['BP'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.rp=-2.5*np.log10(10**(-mist.mag['RP'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.j=-2.5*np.log10(10**(-mist.mag['J'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.h=-2.5*np.log10(10**(-mist.mag['H'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.k=-2.5*np.log10(10**(-mist.mag['K'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.w1=-2.5*np.log10(10**(-mist.mag['W1'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.w2=-2.5*np.log10(10**(-mist.mag['W2'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.w3=-2.5*np.log10(10**(-mist.mag['W3'](mass, age, feh, distance, AV)/2.5))\n",
    "clx.teff=mist.Teff(mass, age, feh)\n",
    "clx.logg=mist.logg(mass, age, feh)\n",
    "clx.logl=mist.logL(mass, age, feh)\n",
    "clx.radius=mist.radius(mass, age, feh)\n",
    "clx.parallax=1000/distance\n",
    "\n",
    "clx = clx[np.where((np.isfinite(clx.g)==True) & (np.isfinite(clx.teff)==True) & \n",
    "                   np.isfinite(clx.logg)==True)[0]]\n",
    "\n",
    "clx['g'][np.where(clx.g>19)[0]]=float('NaN')#what is this doing?\n",
    "clx['bp'][np.where(clx.bp>20.5)[0]]=float('NaN')#I think this is just saying that there should\n",
    "clx['rp'][np.where(clx.rp>17.5)[0]]=float('NaN')#be some values where these terms cannot be\n",
    "clx['j'][np.where(clx.j>17)[0]]=float('NaN')\n",
    "clx['h'][np.where(clx.h>16)[0]]=float('NaN')\n",
    "clx['k'][np.where(clx.k>16)[0]]=float('NaN')\n",
    "clx['w1'][np.where(clx.w1>16)[0]]=float('NaN')\n",
    "clx['w2'][np.where(clx.w2>16)[0]]=float('NaN')\n",
    "clx['w3'][np.where(clx.w3>13.5)[0]]=float('NaN')\n",
    "clx=clx[np.where(np.isfinite(clx.g)==True)[0]]\n",
    "clx['bp'][np.where(np.isfinite(clx.bp)==False)[0]]=21\n",
    "clx['rp'][np.where(np.isfinite(clx.rp)==False)[0]]=18\n",
    "clx['j'][np.where(np.isfinite(clx.j)==False)[0]]=17.5\n",
    "clx['h'][np.where(np.isfinite(clx.h)==False)[0]]=16.5\n",
    "clx['k'][np.where(np.isfinite(clx.k)==False)[0]]=16.5\n",
    "clx['w1'][np.where(np.isfinite(clx.w1)==False)[0]]=16.5\n",
    "clx['w2'][np.where(np.isfinite(clx.w2)==False)[0]]=16.5\n",
    "clx['w3'][np.where(np.isfinite(clx.w3)==False)[0]]=14\n",
    "clx['radius'][np.where(clx.radius<0.5)[0]]=0.5\n",
    "clx['logl'][np.where(clx.logl<-1.54)[0]]=-1.54\n",
    "k=np.where(clx.g-5*(np.log10(1000/clx.parallax)-1)>10)[0]\n",
    "clx['radius'][k]=0.5\n",
    "clx['logl'][k]=-1.54\n",
    "size=len(clx)\n",
    "\n",
    "clusterx = torch.Tensor(size, 1, 12)\n",
    "clustery = torch.Tensor(size, 6)\n",
    "for a in range(size):\n",
    "    clusterx[a][0][0]=.5\n",
    "    clusterx[a][0][1]=.5\n",
    "    clusterx[a][0][2]=.5\n",
    "    clusterx[a][0][3]=.5\n",
    "    clusterx[a][0][4]=.5\n",
    "    clusterx[a][0][5]=.5\n",
    "    clusterx[a][0][6]=.5\n",
    "    clusterx[a][0][7]=.5#why .5?\n",
    "    clusterx[a][0][8]=.5\n",
    "    clusterx[a][0][9]=.5\n",
    "    \n",
    "    clusterx[a][0][0]=clx['g'][a]/21-.5#I don't really understand what all these weights are\n",
    "    clusterx[a][0][1]=clx['bp'][a]/21-.5\n",
    "    clusterx[a][0][2]=clx['rp'][a]/18-.5\n",
    "    clusterx[a][0][3]=clx['j'][a]/17.5-0.5\n",
    "    clusterx[a][0][4]=clx['h'][a]/16.5-0.5\n",
    "    clusterx[a][0][5]=clx['k'][a]/16.5-0.5\n",
    "    clusterx[a][0][6]=clx['w1'][a]/16.5-0.5\n",
    "    clusterx[a][0][7]=clx['w2'][a]/16.5-0.5\n",
    "    clusterx[a][0][8]=clx['w3'][a]/14-0.5\n",
    "    clusterx[a][0][9]=clx['parallax'][a]/20-0.5\n",
    "    clusterx[a][0][10]=(clx.radius[a]-0.2)/5-0.5\n",
    "    clusterx[a][0][11]=clx.logl[a]/4\n",
    "    clustery[a][0]=(clx.age[a]-6)/2-0.5\n",
    "    clustery[a][1]=clx.av[a]/20-0.5\n",
    "    clustery[a][2]=clx.mass[a]/3-0.5\n",
    "    clustery[a][3]=(np.log10(clx.teff[a])-3.4)/0.7-0.5\n",
    "    clustery[a][4]=(clx.logg[a]-3)/2-0.5\n",
    "    clustery[a][5]=(clx.feh[a]+2.5)/3-0.5\n",
    "    \n",
    "# pickle.dump([clusterx, clustery], open('ysotrainf3ms.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our neural network, taking twelve inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape = (1,12)):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,8,3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(8,16,3, padding = 1)\n",
    "        self.conv3 = nn.Conv1d(16,32,3,padding =1)\n",
    "        n_size =self._get_conv_output(input_shape)\n",
    "        self.fc1 = nn.Linear(n_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 6)\n",
    "    \n",
    "    #Generate input sample and forward to get shape\n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = torch.rand(bs, *shape)\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.conv2(F.relu(self.conv1(x)))),2)\n",
    "        x = F.relu(F.max_pool1d(self.conv3(x), 2))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "#Initialize model\n",
    "BAD_LOSS= 100000000\n",
    "model = Net()#initialize our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model=model.to(device)\n",
    "clusterx = clusterx.to(device)\n",
    "clustery = clustery.to(device)\n",
    "clusterx1 = clusterx1.to(device)\n",
    "clustery1 = clustery1.to(device)\n",
    "clusterx2 = clusterx2.to(device)\n",
    "clustery2 = clustery2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88089, 1, 12])\n",
      "torch.Size([23500, 1, 12])\n",
      "tensor([[ 0.0303,  0.0283, -0.0191,  0.0382, -0.0018, -0.0234],\n",
      "        [ 0.0312,  0.0301, -0.0180,  0.0387, -0.0037, -0.0237],\n",
      "        [ 0.0314,  0.0283, -0.0198,  0.0389, -0.0017, -0.0232],\n",
      "        ...,\n",
      "        [ 0.0307,  0.0282, -0.0189,  0.0379, -0.0024, -0.0235],\n",
      "        [ 0.0305,  0.0283, -0.0194,  0.0386, -0.0016, -0.0233],\n",
      "        [ 0.0306,  0.0284, -0.0190,  0.0383, -0.0020, -0.0235]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs = clusterx\n",
    "target = clustery\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5, momentum = .9)\n",
    "\n",
    "best_loss = BAD_LOSS\n",
    "running_dev_loss = 0\n",
    "badcount = 0\n",
    "t = time.time()\n",
    "\n",
    "a = torch.rand(23500, 1, 12)\n",
    "print(clusterx.size())\n",
    "print(a.size())\n",
    "print(model(clusterx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following would be used to start the process, but we want to resume instead"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clusterx.requires_grad=True\n",
    "clustery.requires_grad = True\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5, momentum = .9)\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "ran = range(85000)\n",
    "n=0\n",
    "\n",
    "for epoch in range(1000):\n",
    "# for epoch in range(100000):\n",
    "    model.train()\n",
    "    t = time.time()\n",
    "    k=random.sample(ran, len(ran))\n",
    "    running_loss = 0\n",
    "    for i in range(850):\n",
    "        inputs = clusterx[k[i*100:(i+1)*100]]\n",
    "        target = clustery[k[i*100:(i+1)*100]]\n",
    "        optimizer.zero_grad()#zero parameter gradients\n",
    "\n",
    "        #forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        getloss = loss.item()\n",
    "        running_loss = getloss + running_loss\n",
    "\n",
    "    n=n+1\n",
    "    if n % 1 == 0:\n",
    "        model.eval()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            dev_outputs = model.forward(clusterx)\n",
    "            dev_loss = criterion(dev_outputs, clustery)\n",
    "        print( (n, time.time()-t, running_loss, dev_loss.item()))\n",
    "    if n % 10 == 0:\n",
    "        torch.save(model.state_dict(), 'intermediatemodel.pt')\n",
    "        \n",
    "        \n",
    "print('Finished Training')\n",
    "print('Best dev loss = %.5f' % running_loss)\n",
    "print('Total run time in minutes = %' % ((time.time - start_time)/60))\n",
    "torch.save(model.state_dict(), 'modelfullparsec2.pt')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume(savept, n=0):\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(savept))\n",
    "    model=model.to(device)    \n",
    "    \n",
    "    clusterx.requires_grad=True\n",
    "    clustery.requires_grad=True\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5, momentum = .9)\n",
    "    start_time = time.time()\n",
    "    ran = range(85000)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        model.train()\n",
    "        t = time.time()\n",
    "        k=random.sample(ran, len(ran))\n",
    "        running_loss = 0\n",
    "        for i in range(850):\n",
    "            inputs = clusterx[k[i*100:(i+1)*100]]\n",
    "            target = clustery[k[i*100:(i+1)*100]]\n",
    "            optimizer.zero_grad()#zero parameter gradients\n",
    "\n",
    "            #forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            getloss = loss.item()\n",
    "            running_loss = getloss + running_loss\n",
    "\n",
    "        n=n+1\n",
    "        if n % 1 == 0:\n",
    "            model.eval()\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                dev_outputs = model.forward(clusterx)\n",
    "                dev_loss = criterion(dev_outputs, clustery)\n",
    "            print( (n, time.time()-t, running_loss, dev_loss.item()))\n",
    "        if n % 10 == 0:\n",
    "            torch.save(model.state_dict(), 'intermediatemodel.pt')\n",
    "            pickle.dump(n, open('ncount.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1351, 9.488616943359375, 10100.341459274292, 10296.072265625)\n",
      "(1352, 9.385923862457275, 10075.572836875916, 10472.673828125)\n",
      "(1353, 9.662151098251343, 9941.562607765198, 10336.3359375)\n",
      "(1354, 10.552769660949707, 9960.607882976532, 10095.548828125)\n",
      "(1355, 10.004202842712402, 9978.80999469757, 10500.771484375)\n",
      "(1356, 9.648190259933472, 10036.41046667099, 11183.5966796875)\n",
      "(1357, 9.958317279815674, 10031.898025035858, 10199.650390625)\n",
      "(1358, 9.457664489746094, 10052.014582633972, 10062.576171875)\n",
      "(1359, 9.807851314544678, 9998.059167385101, 10583.271484375)\n",
      "(1360, 9.784822225570679, 9984.48024559021, 11372.08203125)\n",
      "(1361, 9.36790132522583, 9996.163826942444, 10802.9609375)\n",
      "(1362, 9.37591552734375, 9926.866962909698, 11231.490234375)\n",
      "(1363, 10.189717531204224, 10130.645646095276, 10222.90625)\n",
      "(1364, 9.340978860855103, 9965.798392772675, 10167.283203125)\n",
      "(1365, 9.353975296020508, 10013.18553352356, 10388.587890625)\n",
      "(1366, 9.387884140014648, 9969.673046588898, 10172.70703125)\n",
      "(1367, 9.61926794052124, 9970.308890342712, 10028.24609375)\n",
      "(1368, 9.387853384017944, 10004.180034637451, 10356.404296875)\n",
      "(1369, 9.321019172668457, 10049.410531044006, 10186.0341796875)\n",
      "(1370, 9.581337451934814, 9925.290500164032, 10007.6826171875)\n",
      "(1371, 10.159289598464966, 9956.41306066513, 10114.59375)\n",
      "(1372, 9.320319890975952, 9973.329549789429, 10355.068359375)\n",
      "(1373, 9.418803215026855, 9947.85293006897, 10352.984375)\n",
      "(1374, 9.595331192016602, 10033.85184621811, 10482.2177734375)\n",
      "(1375, 9.46268367767334, 10063.454723834991, 10188.962890625)\n",
      "(1376, 9.385891199111938, 9907.363845825195, 10743.94921875)\n",
      "(1377, 9.382863759994507, 9968.048718452454, 10170.2431640625)\n",
      "(1378, 9.532498598098755, 9961.62089061737, 10343.8486328125)\n",
      "(1379, 9.388959884643555, 9946.452638626099, 10226.44921875)\n",
      "(1380, 9.403920412063599, 9938.104894638062, 10124.0068359375)\n",
      "(1381, 9.571394920349121, 9928.733371257782, 9990.5703125)\n",
      "(1382, 9.409828186035156, 9949.326242923737, 10070.4013671875)\n",
      "(1383, 9.278181314468384, 9958.085943698883, 10583.5439453125)\n",
      "(1384, 9.291349172592163, 9965.13611459732, 10076.58984375)\n",
      "(1385, 9.296136140823364, 9987.32188987732, 10167.8095703125)\n",
      "(1386, 9.301117658615112, 9982.668243408203, 10171.2421875)\n",
      "(1387, 9.396939277648926, 9934.99185705185, 10085.6337890625)\n",
      "(1388, 9.359995603561401, 9933.358977794647, 9922.869140625)\n",
      "(1389, 9.410824537277222, 9898.052202224731, 10046.083984375)\n",
      "(1390, 9.337054967880249, 9963.168389320374, 10169.013671875)\n",
      "(1391, 9.316104888916016, 10017.876425743103, 10383.322265625)\n",
      "(1392, 9.303074359893799, 9926.191510677338, 10416.736328125)\n",
      "(1393, 9.292108058929443, 9900.997236728668, 10072.501953125)\n",
      "(1394, 9.313085079193115, 10011.903907775879, 10490.708984375)\n",
      "(1395, 9.346997261047363, 9889.811971187592, 10410.73828125)\n",
      "(1396, 9.289151191711426, 9931.795517921448, 11287.0986328125)\n",
      "(1397, 9.32605242729187, 9992.700229167938, 9910.37890625)\n",
      "(1398, 9.358927249908447, 9937.941922187805, 9999.25)\n",
      "(1399, 9.352985858917236, 9920.197360515594, 10107.826171875)\n",
      "(1400, 9.26122522354126, 9950.477365493774, 11583.619140625)\n",
      "(1401, 9.287156820297241, 10018.196597576141, 10373.6748046875)\n",
      "(1402, 9.49759292602539, 9856.74057340622, 10254.220703125)\n",
      "(1403, 9.550529479980469, 9971.72817993164, 9970.521484375)\n",
      "(1404, 9.588350772857666, 9919.112561702728, 10539.521484375)\n",
      "(1405, 9.526514291763306, 9861.783107280731, 10712.8525390625)\n",
      "(1406, 9.520530462265015, 9965.41486787796, 10389.50390625)\n",
      "(1407, 9.480637550354004, 9867.074696063995, 10305.6484375)\n",
      "(1408, 9.435763120651245, 10029.385933876038, 10135.6982421875)\n",
      "(1409, 9.428776502609253, 9917.90702533722, 10343.8623046875)\n",
      "(1410, 9.4148108959198, 9929.90035533905, 10124.3017578125)\n",
      "(1411, 9.294134855270386, 9914.699674606323, 9873.087890625)\n",
      "(1412, 9.530502557754517, 9895.502446651459, 10133.1923828125)\n",
      "(1413, 9.449717998504639, 9896.217245101929, 10350.5029296875)\n",
      "(1414, 9.364923000335693, 9857.18907403946, 10032.5771484375)\n",
      "(1415, 9.371928453445435, 10060.47106552124, 10189.4921875)\n",
      "(1416, 9.400850057601929, 9875.480110645294, 10329.865234375)\n",
      "(1417, 9.421794652938843, 9935.927296161652, 10142.2255859375)\n",
      "(1418, 9.505538940429688, 10025.388703346252, 10192.1201171875)\n",
      "(1419, 9.424751281738281, 9963.43593454361, 10258.767578125)\n",
      "(1420, 9.385881900787354, 9828.039268493652, 10365.421875)\n",
      "(1421, 9.2821683883667, 9930.430588722229, 10344.810546875)\n",
      "(1422, 9.309096813201904, 9930.446881771088, 10013.552734375)\n",
      "(1423, 9.32103180885315, 9872.290176868439, 10433.40234375)\n",
      "(1424, 9.303112506866455, 9906.924139976501, 10145.6962890625)\n",
      "(1425, 9.304073333740234, 9962.708482265472, 9928.2890625)\n",
      "(1426, 9.393837451934814, 9843.47871351242, 10043.5283203125)\n",
      "(1427, 9.370930433273315, 9886.542129993439, 10231.5)\n",
      "(1428, 9.394866943359375, 9917.017907619476, 10252.0107421875)\n",
      "(1429, 9.539442539215088, 9932.861910820007, 11025.060546875)\n",
      "(1430, 9.464657068252563, 9885.283742427826, 10692.68359375)\n",
      "(1431, 9.46368145942688, 9908.612257957458, 10141.841796875)\n",
      "(1432, 9.526515007019043, 9834.985524654388, 10077.578125)\n",
      "(1433, 9.278179168701172, 9851.3867893219, 10918.37890625)\n",
      "(1434, 9.403843402862549, 9935.389291286469, 10200.537109375)\n",
      "(1435, 9.339978456497192, 10029.99470281601, 9935.1455078125)\n",
      "(1436, 9.43176555633545, 9839.703557491302, 10738.857421875)\n",
      "(1437, 9.39286994934082, 9881.098586082458, 9833.697265625)\n",
      "(1438, 9.365944385528564, 9883.320758342743, 10469.5634765625)\n",
      "(1439, 9.335026025772095, 9865.01671552658, 9768.07421875)\n",
      "(1440, 9.361955165863037, 9757.70604467392, 9810.982421875)\n",
      "(1441, 9.36694049835205, 9774.91057729721, 10126.1201171875)\n",
      "(1442, 9.792385339736938, 9945.305886268616, 9916.0458984375)\n",
      "(1443, 9.383883237838745, 9835.830020904541, 10418.10546875)\n",
      "(1444, 9.380904197692871, 9905.769567489624, 10634.3056640625)\n",
      "(1445, 9.351946115493774, 9849.313781738281, 10019.283203125)\n",
      "(1446, 9.367937088012695, 9878.191471099854, 10266.4755859375)\n",
      "(1447, 10.245591878890991, 9878.090590000153, 10171.98828125)\n",
      "(1448, 12.999193906784058, 9851.022817134857, 10298.0908203125)\n",
      "(1449, 11.84228801727295, 9831.301494121552, 10234.4208984375)\n",
      "(1450, 11.762566089630127, 9877.66465806961, 10527.185546875)\n",
      "(1451, 11.637858152389526, 9818.658822536469, 10424.021484375)\n",
      "(1452, 10.571445226669312, 9887.539863586426, 9823.59375)\n",
      "(1453, 10.956689357757568, 9784.966879367828, 10215.521484375)\n",
      "(1454, 11.812399625778198, 9876.31157541275, 9976.3203125)\n",
      "(1455, 9.687082767486572, 9870.52838563919, 10140.2607421875)\n",
      "(1456, 9.343005895614624, 9836.744609832764, 10029.73046875)\n",
      "(1457, 9.299089670181274, 9797.704955101013, 9964.98046875)\n",
      "(1458, 9.304112911224365, 9926.454895019531, 9865.67578125)\n",
      "(1459, 9.356967210769653, 9918.12465429306, 10522.79296875)\n",
      "(1460, 9.431941747665405, 9933.744210243225, 9947.359375)\n",
      "(1461, 9.316040992736816, 9803.042935371399, 9997.8994140625)\n",
      "(1462, 9.345998287200928, 9823.639219760895, 10175.9013671875)\n",
      "(1463, 9.298124074935913, 9835.144656181335, 10124.87109375)\n",
      "(1464, 9.290144443511963, 9874.844392299652, 9660.35546875)\n",
      "(1465, 9.367053985595703, 9812.094942569733, 10297.591796875)\n",
      "(1466, 9.3290433883667, 9871.85204076767, 10052.25390625)\n",
      "(1467, 9.340013265609741, 9754.761145591736, 10127.0185546875)\n",
      "(1468, 9.317074537277222, 9895.13468503952, 10004.00390625)\n",
      "(1469, 9.286161422729492, 9835.611450195312, 10597.28515625)\n",
      "(1470, 9.403843402862549, 9895.388140201569, 9889.044921875)\n",
      "(1471, 9.41877031326294, 9808.315669059753, 9980.193359375)\n",
      "(1472, 9.34500503540039, 9775.805350780487, 10389.36328125)\n",
      "(1473, 9.38788652420044, 9870.10086774826, 10802.380859375)\n",
      "(1474, 9.287261724472046, 9766.724930286407, 9850.580078125)\n",
      "(1475, 9.256238222122192, 9783.156865119934, 10448.58984375)\n",
      "(1476, 9.228312730789185, 9905.708763122559, 10184.4716796875)\n",
      "(1477, 9.321032285690308, 9818.4505443573, 10331.533203125)\n",
      "(1478, 9.285160303115845, 9933.332348823547, 10055.15625)\n",
      "(1479, 9.898520231246948, 9773.95589542389, 9900.4775390625)\n",
      "(1480, 9.645198345184326, 9842.211040019989, 10285.111328125)\n",
      "(1481, 9.628210544586182, 9805.047934055328, 10219.8359375)\n",
      "(1482, 10.291470289230347, 9797.050977230072, 10556.064453125)\n",
      "(1483, 11.651909351348877, 9777.979922294617, 10030.703125)\n",
      "(1484, 9.641207695007324, 9739.20423078537, 10135.595703125)\n",
      "(1485, 9.74991512298584, 9807.099105358124, 10122.994140625)\n",
      "(1486, 12.046772241592407, 9814.494852542877, 9852.17578125)\n",
      "(1487, 9.84167194366455, 9756.20928478241, 10241.759765625)\n",
      "(1488, 9.383895874023438, 9876.160547733307, 10401.49609375)\n",
      "(1489, 10.418504238128662, 9807.581864356995, 10373.9873046875)\n"
     ]
    }
   ],
   "source": [
    "N = pickle.load(open('ncount.pickle', 'rb'))\n",
    "resume('intermediatemodel.pt', n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
